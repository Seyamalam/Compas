\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Fine-Tuning Llama2 Inference: A Comparative Exploration of
Language Implementations for Optimal Efficiency
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Abhijit Pathak}
\IEEEauthorblockA{\textit{Assistant Professor} \\
\textit{BGC Trust University Bangladesh}\\
Chattogram, Bangladesh \\
abhijitpathak@bgctub.ac.bd \\
0000-0001-7734-0271}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Touhidul Alam Seyam}
\IEEEauthorblockA{\textit{Research Assistant} \\
\textit{BGC Trust University Bangladesh}\\
Chattogram, Bangladesh \\
touhidulalam@bgctub.ac.bd \\
0009-0007-7512-1893}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Arnab Chakraborty}
\IEEEauthorblockA{\textit{Research Assistant}\\
\textit{BGC Trust University Bangladesh}\\
Chattogram, Bangladesh \\
arnabchakrabortybd@gmail.com\\
0009-0008-0279-7187}
\and
\IEEEauthorblockN{4\textsuperscript{th} Arnab Chakraborty}
\IEEEauthorblockA{\textit{Research Assistant}\\
\textit{BGC Trust University Bangladesh}\\
Chattogram, Bangladesh \\
arnabchakrabortybd@gmail.com\\
0009-0008-0279-7187}
\and
\IEEEauthorblockN{5\textsuperscript{th} Eftekhar}
\IEEEauthorblockA{\textit{Research Assistant} \\
\textit{BGC Trust University Bangladesh}\\
Chattogram, Bangladesh \\
minhaj.im87@gmail.com \\
0009-0001-8744-4581}
\and
\IEEEauthorblockN{6\textsuperscript{th} Mim}
\IEEEauthorblockA{\textit{Research Assistant}\\
\textit{BGC Trust University Bangladesh }\\
Chattogram, Bangladesh\\
tibrapaul4@gmail.com\\
0009-0007-9700-2350}
}

\maketitle

\begin{abstract}
    This paper conducts a comparative investigation to maximize the effectiveness of Llama2 inference, a critical task in machine learning and natural language processing (NLP). Various programming languages and frameworks, including TensorFlow, PyTorch, Python, Mojo, C++, and Java, are examined, assessing their speed, memory consumption, and ease of implementation through extensive testing and benchmarking. The advantages and disadvantages of each strategy are noted, with suggested optimization methods for parallel processing and hardware utilization. Additionally, the performance of the Mojo SDK, a novel framework designed for LLM inference on Apple Silicon, is investigated, comparing it against established implementations in C, C++, Rust, Zig, Go, and Julia. Through comprehensive benchmarking on an Apple M1 Max, Mojo SDK's competitive performance and its advantages in ease of use and Python compatibility are demonstrated, suggesting it is a compelling alternative for LLM inference on Apple Silicon. Implications for the future of LLM deployment on resource-limited hardware and potential avenues for further research are discussed.
\end{abstract}

\begin{IEEEkeywords}
    Large Language Model (LLM), Inference, Llama2, Mojo, Rust, NLP
\end{IEEEkeywords}

\section{Introduction}
In NLP, finetuning large-scale language models like Llama2 is essential for high performance across various tasks. However, optimizing Llama2 inference efficiency in resource-constrained environments remains critical. The choice of language implementation significantly affects this optimization, yet there is limited knowledge on its impact. This study addresses this gap by comparing the efficiency of different language implementations for finetuning Llama2 inference.

The research question is: How do different language implementations affect the efficiency of finetuning Llama2 inference, and which offers the best balance between performance and resource utilization? The study evaluates Python, C++, and Mojo, assessing performance indicators like inference speed, memory usage, and computational resources.

Llama2, developed by Meta AI, is notable for its features and open-source accessibility, but efficient inference often requires specialized hardware. The Mojo SDK by Modular AI, designed for machine learning, offers a solution for efficient LLM inference on Apple Silicon, combining low-level performance with Python's usability.

This paper benchmarks Mojo SDK against other languages, analyzing tokens per second, inference time, and memory usage. Results show Mojo SDK achieves competitive performance with significant advantages in ease of use and development efficiency. This research aims to guide practitioners in optimizing language implementations for Llama2, enhancing NLP systems and applications.


\section{Background Study}
As described in this study, MeZO demonstrates that proper pre-training and task prompts allow MeZO to finetune large models [5]. MeZO adapts the traditional ZO-SGD approach to run in-place, finetuning LLMs with the same memory footprint as inference. By incorporating adaptive signals while maintaining pre-trained information, LLaMA-Adapter effectively finetunes LLaMA with few parameters, enabling higher reasoning performance in multi-modal tasks and high-quality replies [8]. The authors highlight how learner modules and priming take advantage of the overparameterization of pre-trained language models to improve the resource consumption and convergence time of BERT-based models [6]. The authors discuss Inference-Time Intervention (ITI), a technique designed to enhance the integrity of Large Language Models (LLMs) by rearranging model activations during inference, distributing attention among a restricted number of heads in a certain way [3]. To supplement Pre-trained Language Models (PLMs) with textual representations of PLMs in two stages, this article reviewed kNN classifiers: The training procedure is calibrated by (1) using kNN as prior knowledge and (2) linearly interpolating the probability distribution predicted by kNN with the PLMs classifier [4]. As stated in this study, LlamaTune uses bucketization of knob values, a biased sampling technique to handle unique values for specific knobs, and an automated dimensionality reduction technique based on randomized projections to minimize the size of the search space [2]. The authors highlight how the delta-tuning strategy significantly reduces compute and storage costs by optimizing a tiny section of the model parameters while leaving the rest constant. It also shows how large-scale models could be successfully stimulated by enhancing a select few parameters [1]. To enable finetuning LMs with limited supervision, this study builds COSINE, a contrastive self-training framework supported by contrastive regularization and confidence-based reweighting that progressively enhances model fitting while successfully suppressing error propagation [7].


\section{Methodology}
The methodology of this work compares Llama2 inference's efficacy across different programming languages and frameworks using a methodical technique. A thorough evaluation and benchmarking process is carried out on TensorFlow, PyTorch, Python, Mojo, C++, and Java, taking into account variables like performance, memory usage, and simplicity of use. The Mojo SDK's performance on Apple Silicon is also carefully assessed by contrasting it with well-known implementations in C, C++, Rust, Zig, Go, and Julia. Insights regarding the effectiveness and viability of each strategy are obtained through thorough benchmarking on an Apple M1 Max, helping to shape optimization techniques and emphasizing the competitive advantages of Mojo SDK, especially its compatibility with Python and simplicity of use on Apple Silicon hardware.

\begin{figure}[h]
    \centerline{\includegraphics[width=\linewidth]{fig1.png}}
    \caption{Benchmarking Process for Evaluating Llama2 Implementations across Model Sizes and Threading Configurations.}
    \label{fig1}
\end{figure}

The flowchart (Figure 1) outlines a benchmarking process for evaluating different implementations of the Llama2 system across various model sizes and threading configurations. It starts by defining model sizes and selecting implementations in other programming languages. Then, it splits into two paths to run single-threaded and multi-threaded benchmarks using the hyper-tune tool. After collecting performance metrics like TPS, inference time, and memory usage, the results are analyzed to compare implementations across models and threading. Finally, insights are drawn from the analysis to conclude the benchmarking process.

\subsection{Testing Environment and Hardware}
The benchmarking was carried out on a MacBook Pro outfitted with an Apple M2 Max SoC, boasting a 10-core CPU, a 32-core GPU, and a 16-core Neural Engine. All tests were conducted solely in CPU-only mode to ensure consistency and isolate the performance of the language implementations.

\subsection{Benchmarking Tools and Framework}
To ensure reliable and comparable performance measurements, we implemented a custom benchmarking framework designed to execute LLM inference tasks consistently across all language implementations. We utilized the \texttt{Hypertune} tool, a fork of the popular hyperfine command-line benchmarking utility, with enhanced features for granular performance data capture. This allowed us to measure and record critical metrics, including tokens per second, time per inference, and memory usage.

\subsection{Performance Metrics}
Two fundamental performance metrics were employed to assess the efficacy of LLM implementations: tokens per second and time per inference. Tokens per second quantify the processing speed, reflecting the model’s throughput, while time per inference measures the average duration for completing a single inference step, indicative of responsiveness and latency.

\subsection{Testing Process}

\subsubsection{Single-Threaded and Multi-Threaded Configurations}
Each language implementation underwent testing in both single-threaded and multi-threaded configurations, where feasible, to assess its scalability and efficiency in leveraging available CPU cores. Multi-threaded tests were conducted with varying thread counts to explore the impact of parallel processing on performance.

\subsubsection{Model Conversion}
To ensure equitable comparison across implementations, the Llama2 models were converted to the fp32 GGUF format utilizing the llama.cpp converter tool. This step was imperative to accommodate potential differences in model format requirements across implementations—subsequently, each implementation loaded and executed the converted models for benchmarking purposes.

\subsubsection{Inference Execution}
A series of inference tasks were executed using each implementation and model combination, with performance metrics recorded for subsequent analysis—these tasks involved presenting prompts and generating text completions, mirroring real-world LLM usage scenarios. Collected results were meticulously analyzed to discern and compare the performance characteristics of each implementation.


\subsection{Data Collection and Analysis}
The amassed performance data underwent rigorous scrutiny, employing statistical techniques and visualization tools to uncover nuanced insights. Through comparative analysis, trends in performance across different implementations were elucidated, allowing for a comprehensive evaluation of their strengths and weaknesses. The findings of this analysis served as the basis for drawing meaningful conclusions re- regarding the efficacy and suitability of Mojo SDK for LLM inference tasks on Apple Silicon.

\begin{figure}[h]
    \centerline{\includegraphics[width=\linewidth]{fig2.png}}
    \caption{Overview Process for Evaluating LLM Implementations.}
    \label{fig2}
\end{figure}

\section{Results And Discussion}
The performance evaluation of LLM implementations (Figure 3) provides crucial insights into their efficiency and suitability for various tasks. By assessing metrics such as throughput, latency, and resource utilization across different languages and frameworks, re-researchers can identify optimal solutions for LLM inference, informing developers and researchers about the most effective approaches for their applications.

\subsection{Multi-Threaded Performance Comparison}
The multi-threaded benchmarks conducted by the authors provide valuable insights into the performance of Mojo SDK and other language implementations for Llama2 inference. Key findings include:

\begin{itemize}
    \item Mojo SDK consistently demonstrates competitive performance across all model sizes, indicating its effectiveness regarding tokens per second and inference time. While not always the top performer, Mojo's Python-like usability and low-level optimization combination contribute to efficient LLM inference. Moreover, Mojo scales well with larger models, narrowing the performance gap with other implementations.
    \begin{figure}[h]
        \centerline{\includegraphics[width=\linewidth]{fig3.png}}
        \caption{Overview Process for Evaluating LLM Implementations.}
        \label{fig2}
    \end{figure}
    \item C++ and Zig implementations exhibit strong performance, which is particularly noticeable in larger models. This underscores their suitability for computationally intensive tasks and effective utilization of modern hardware capabilities.
    \item Go, Rust, and Julia perform moderately, offering reasonable efficiency for LLM inference tasks. Additional optimizations and platform-specific tuning could further enhance their performance.

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.7cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={C++, Mojo, Zig, C, Rust, Julia, Go},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=1100,
                ]
                \addplot coordinates {(900.4463,C++) (885.71,Mojo) (702.3167,Zig) (531.6647,C) (531.6647,Rust) (320.887,Julia) (185.5577,Go)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories15M.bin model}
      \end{figure}
      
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.7cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Julia, Go, Rust, C++, Zig, C, Mojo},
                ytick=data,
                xbar,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=3,
                ]
                \addplot coordinates {(2.48412,Julia) (1.306325,Go) (0.802,Rust) (0.5592776,C++) (0.4834227,Zig) (0.4373009,C) (0.325698,Mojo)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference for stories15M.bin model}
      \end{figure}
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.7cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Mojo, C++, C, Zig, Rust, Julia, Go},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=500,
                ]
                \addplot coordinates {(395.088,Mojo) (384.4397,C++) (270.0187,C) (268.2443,Zig) (193.8413,Rust) (181.1473,Julia) (78.60167,Go)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories42M.bin model}
      \end{figure}
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.7cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Julia, Go, Rust, Zig, C++, C, Mojo},
                ytick=data,
                xbar,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=3.5,
                ]
                \addplot coordinates {(2.939204,Julia) (2.912359,Go) (1.393408,Rust) (0.982617,Zig) (0.9472967,C++) (0.8171151,C) (0.6637589,Mojo)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference for stories42M.bin model}
      \end{figure}
      
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={C++, Mojo, C, Zig, Rust, Julia},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=250,
                ]
                \addplot coordinates {(174.5473,C++) (160.132,Mojo) (122.4277,C) (100.0353,Zig) (98.8087,Rust) (89.77033,Julia)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories110M.bin model}
      \end{figure}
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Go, Julia, Rust, Zig, C, Mojo, C++},
                ytick=data,
                xbar,
                xmin=0, xmax=25,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=10,
                ]
                \addplot coordinates {(7.653778,Go) (4.540055,Julia) (2.754481,Rust) (2.664162,Zig) (2.02678,C) (1.775346,Mojo) (1.775346,C++)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference for stories110M.bin model.}
      \end{figure}


    \item Mojo SDK consistently demonstrates competitive performance across all model sizes, indicating its effectiveness regarding tokens per second and inference time. While not always the top performer, Mojo's Python-like usability and low-level optimization combination contribute to efficient LLM inference. Moreover, Mojo scales well with larger models, narrowing the performance gap with other implementations.
      \item C++ and Zig implementations exhibit strong performance, which is particularly noticeable in larger models. This underscores their suitability for computationally intensive tasks and effective utilization of modern hardware capabilities.
      \item Go, Rust, and Julia perform moderately, offering reasonable efficiency for LLM inference tasks. Additional optimizations and platform-specific tuning could further enhance their performance.
\end{itemize}


\subsection{Single-Threaded Performance Comparison}
Single-threaded benchmarks provide a valuable perspective on the inherent efficiency of each implementation, independent of multi-threading capabilities. The results are summarized below:

\begin{itemize}

    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Zig, C, C++, Mojo, Rust, Julia, Go},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=850,
                ]
                \addplot coordinates {(707.5707,Zig) (700.4067,C) (697.0493,C++) (604.7863,Mojo) (385.6671,Rust) (322.2083,Julia) (65.466,Go)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories15M.bin model}
      \end{figure}
      \item C implementations excel in single-threaded scenarios with the llama2.c implementation consistently achieves high tokens per second and low inference times across all model Sizes. This highlights C's effectiveness for highly optimized, single-threaded tasks.
      \item Mojo SDK maintains single-threaded solid performance, surpassing several other implementations while not matching the speed of C. This underscores the efficiency of Mojo’s design and compilation strategy.
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Go, Julia, Rust, C++, Zig, Mojo, C},
                ytick=data,
                xbar,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=4,
                ]
                \addplot coordinates {(3.487985,Go) (2.418289,Julia) (0.7912604,Rust) (0.6434277,C++) (0.4833996,Zig) (0.4374733,Mojo) (0.334866,C)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference for stories15M.bin model}
      \end{figure}
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={C, Zig, C++, Mojo, Rust, Julia},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=350,
                ]
                \addplot coordinates {(268.9483,C) (268.1983,Zig) (265.3133,C++) (203.529,Mojo) (193.8467,Rust) (181.22,Julia)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories42M.bin model}
      \end{figure}
      
      \begin{figure}[h]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Go, Julia, Rust, C++, Mojo, Zig, C},
                ytick=data,
                xbar,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=3.5,
                ]
                \addplot coordinates {(8.589452,Go) (2.930905,Julia) (1.393492,Rust) (1.258079,C++) (1.172187,Mojo) (0.984326,Zig) (0.8232438,C)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference  for stories42M.bin model}
      \end{figure}
      
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={C++, C, Zig, Julia, Rust, Mojo, Go},
                ytick=data,
                xbar,
                xlabel={Average Tokens per Second},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=150,
                ]
                \addplot coordinates {(104.0357,C++) (101.0133,C) (99.915,Zig) (90.48933,Julia) (99.4693,Rust) (72.46933,Mojo) (10.39367,Go)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Tokens per Second for stories110M.bin model}
      \end{figure}
      
      \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                y=0.8cm,
                bar width=0.4cm,
                width=\linewidth,
                height=8cm,
                symbolic y coords={Go, Julia, Mojo, C++, Rust, Zig, C},
                ytick=data,
                xbar,
                xlabel={Average Time per Inference (seconds)},
                nodes near coords,
                nodes near coords align={horizontal},
                xmin=0,
                xmax=30,
                ]
                \addplot coordinates {(23.69974,Go) (4.52782,Julia) (3.595333,Mojo) (2.799485,C++) (2.738403,Rust) (2.668282,Zig) (2.4586,C)};
            \end{axis}
        \end{tikzpicture}
        \caption{Average Time per Inference for stories110M.bin model}
      \end{figure}

      \item Zig performs across single-threaded and multi-threaded configurations, suggesting efficient core logic and minimal overhead threading.
\end{itemize}
The results affirm Mojo SDK as a compelling choice for Llama2 inference on Apple Silicon, offering competitive performance with optimized implementations in C and C++, alongside greater ease of use and compatibility with the Python ecosystem. However, performance may vary based on hardware configurations, model sizes, and inference tasks. Further optimization and platform-specific tuning can enhance performance across all implementations, including Mojo SDK. The benchmarking results underscore Mojo SDK's effectiveness as an efficient solution for Llama2 inference on Apple Silicon. Its competitive performance, ease of use, and Python compatibility are unique advantages in the LLM development landscape. Leveraging the extensive Python ecosystem for pre-processing, post-processing, and integration with other machine-learning tools enhances Mojo's appeal to developers seeking performance and productivity.

\subsection{Advantages of Mojo SDK}
Mojo SDK streamlines LLM development with its Python-like syntax and seamless Python library integration, simplifying complexities inherent in lower-level languages like C or C++. This abstraction lets developers focus on model integration and application logic rather than grapple with the intricacies of memory management or performance optimization. Despite its high-level approach, Mojo's compiler infrastructure ensures optimal performance, often rivalling or surpassing established implementations. Its accessibility to Python developers broadens its user base, while an active community fosters collaboration and accelerates technology adoption.

\subsection{Limitations and Future Work}
While this study focused on CPU-based inference on Apple Sili- con, future research could explore the impact of GPU acceleration On performance, it is essential to consider Mojo's adaptability to diverse hardware. Additionally, it would enhance its capabilities and limitations by investigating Mojo SDK's performance and optimization across different hardware platforms with varying architectures or resource constraints. Moreover, the emergence of efficient and user-friendly LLM inference solutions like Mojo SDK has significant implications for the future of LLM deployment. Lowering the barrier to entry and enabling efficient execution on resource-constrained devices, Mojo empowers a broader range of developers and researchers to explore the potential of LLMs in various applications. This could lead to accelerated innovation in edge computing, personalized AI assistants, and LLM-powered mobile applications.

\section{Conclusion}
In conclusion, this study highlights Mojo SDK as a compelling solution for efficient and accessible Llama2 inference on Apple Silicon. Its competitive performance, ease of use, and Python compatibility make it a valuable tool for developers and researchers. By simplifying development processes and enabling efficient execution on resource-constrained devices, Mojo SDK has the potential to democratize access to powerful language models, fostering innovation across various domains. Looking ahead, further exploration of Mojo's capabilities with GPU acceleration, diverse hardware platforms, and evolving LLM architectures holds promise for the future of LLM deployment. As the LLM landscape evolves, Mojo SDK emerges as a promising framework that empowers a broader range of users to harness the power of LLMs and unlock their transformative potential, driving innovation and advancements in the field.


\begin{thebibliography}{00}
\bibitem{b1} Richard Antonello, Nicole Beckage, Javier Turek, and Alexander Huth. 2020. Select- ing informative contexts improves language model finetuning. arXiv preprint arXiv:2005.00175 (2020).
\bibitem{b2} Konstantinos Kanellis, Cong Ding, Brian Kroth, Andreas Müller, Carlo Curino, and Shivaram Venkataraman. 2022. LlamaTune: Sample-efficient DBMS configuration tuning. arXiv preprint arXiv:2203.05128 (2022).
\bibitem{b3} Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. Inference-time intervention: Eliciting truthful answers from a language model, july 2023. URL http://arxiv. org/abs/2306.03341 (2023).
\bibitem{b4} Lei Li, Jing Chen, Botzhong Tian, and Ningyu Zhang. 2023a. Revisiting k-NN for Finetuning Pre-trained Language Models. In China National Conference on Chinese Computational Linguistics. Springer, 327–338.
\bibitem{b5} Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. 2024. Finetuning language models with just forward passes. Advances in Neural Information Processing Systems 36 (2024).
\bibitem{b6} Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, and Warren J Gross. 2022. Efficient finetuning of compressed language models with learners. arXiv preprint arXiv:2208.02070 (2022).
\bibitem{b7} Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. 2020. Finetuning pre-trained language model with weak supervision: A contrastive- regularized self-training approach. arXiv preprint arXiv:2010.07835 (2020).
\bibitem{b8} Recruit Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023. Llama-adapter: Efficient finetuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 (2023).
\end{thebibliography}
\vspace{12pt}
\color{red}


\end{document}
