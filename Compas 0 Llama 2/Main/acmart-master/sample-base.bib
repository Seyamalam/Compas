[1]

@article{malladi2024fine,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

[2]
@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

[3]
@article{vucetic2022efficient,
  title={Efficient fine-tuning of compressed language models with learners},
  author={Vucetic, Danilo and Tayaranian, Mohammadreza and Ziaeefard, Maryam and Clark, James J and Meyer, Brett H and Gross, Warren J},
  journal={arXiv preprint arXiv:2208.02070},
  year={2022}
}

[4]
@article{li2023inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model, july 2023},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={URL http://arxiv. org/abs/2306.03341},
  year={2023}
}

[5]
@inproceedings{li2023revisiting,
  title={Revisiting k-NN for Fine-Tuning Pre-trained Language Models},
  author={Li, Lei and Chen, Jing and Tian, Botzhong and Zhang, Ningyu},
  booktitle={China National Conference on Chinese Computational Linguistics},
  pages={327--338},
  year={2023},
  organization={Springer}
}

[6]
@article{kanellis2022llamatune,
  title={LlamaTune: Sample-efficient DBMS configuration tuning},
  author={Kanellis, Konstantinos and Ding, Cong and Kroth, Brian and M{\"u}ller, Andreas and Curino, Carlo and Venkataraman, Shivaram},
  journal={arXiv preprint arXiv:2203.05128},
  year={2022}
}

[7]
@article{antonello2020selecting,
  title={Selecting informative contexts improves language model finetuning},
  author={Antonello, Richard and Beckage, Nicole and Turek, Javier and Huth, Alexander},
  journal={arXiv preprint arXiv:2005.00175},
  year={2020}
}

[8]
@article{yu2020fine,
  title={Fine-tuning pre-trained language model with weak supervision: A contrastive-regularized self-training approach},
  author={Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  journal={arXiv preprint arXiv:2010.07835},
  year={2020}
}

